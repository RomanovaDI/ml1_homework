{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2: Линейные модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <hr\\>\n",
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 5 ноября 2019, 06:00 <br\\>\n",
    "**Штраф за опоздание:** -2 балла после 06:00 5 ноября, -4 балла после 06:00 12 ноября, -6 баллов после 06:00 19 ноября  -8 баллов после 06:00 26 ноября.\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла Присылать ДЗ необходимо в виде ссылки на свой github репозиторий на почту ml1.sphere@mail.ru с указанием темы в следующем формате:\n",
    "[ML0919, Задание 2] Фамилия Имя.<br\\>\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания.\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст, если явно не указана такая возможность. В противном случае -1 балл\n",
    "<hr\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здравствуйте, уважаемые студенты! \n",
    "\n",
    "В этом задании мы будем реализовать линейные модели. Необходимо реализовать линейную и логистическую регрессии с L2 регуляризацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретическое введение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная регрессия решает задачу регрессии и оптимизирует функцию потерь MSE \n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right], $$ где $y_i$ $-$ целевая функция,  $a_i = a(x_i) =  \\langle\\,x_i,w\\rangle ,$ $-$ предсказание алгоритма на объекте $x_i$, $w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Не забываем, что здесь и далее  мы считаем, что в $x_i$ есть тождественный вектор единиц, ему соответствует вес $w_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является линейным классификатором, который оптимизирует так называемый функционал log loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right],$$\n",
    "где  $y_i  \\in \\{0,1\\}$ $-$ метка класса, $a_i$ $-$ предсказание алгоритма на объекте $x_i$. Модель пытается предсказать апостериорую вероятность объекта принадлежать к классу \"1\":\n",
    "$$ p(y_i = 1 | x_i) = a(x_i) =  \\sigma( \\langle\\,x_i,w\\rangle ),$$\n",
    "$w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Функция $\\sigma(x)$ $-$ нелинейная функция, пероводящее скалярное произведение объекта на веса в число $\\in (0,1)$ (мы же моделируем вероятность все-таки!)\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "Если внимательно посмотреть на функцию потерь, то можно заметить, что в зависимости от правильного ответа алгоритм штрафуется или функцией $-\\log a_i$, или функцией $-\\log (1 - a_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто для решения проблем, которые так или иначе связаны с проблемой переобучения, в функционал качества добавляют слагаемое, которое называют ***регуляризацией***. Итоговый функционал для линейной регрессии тогда принимает вид:\n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right] + \\frac{1}{C}R(w) $$\n",
    "\n",
    "Для логистической: \n",
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right] +  \\frac{1}{C}R(w)$$\n",
    "\n",
    "Самое понятие регуляризации введено основателем ВМК академиком Тихоновым https://ru.wikipedia.org/wiki/Метод_регуляризации_Тихонова\n",
    "\n",
    "Идейно методика регуляризации заключается в следующем $-$ мы рассматриваем некорректно поставленную задачу (что это такое можно найти в интернете), для того чтобы сузить набор различных вариантов (лучшие из которых будут являться переобучением ) мы вводим дополнительные ограничения на множество искомых решений. На лекции Вы уже рассмотрели два варианта регуляризации.\n",
    "\n",
    "$L1$ регуляризация:\n",
    "$$R(w) = \\sum_{j=1}^{D}|w_j|$$\n",
    "$L2$ регуляризация:\n",
    "$$R(w) =  \\sum_{j=1}^{D}w_j^2$$\n",
    "\n",
    "С их помощью мы ограничиваем модель в  возможности выбора каких угодно весов минимизирующих наш лосс, модель уже не сможет подстроиться под данные как ей угодно. \n",
    "\n",
    "Вам нужно добавить соотвествущую Вашему варианту $L2$ регуляризацию.\n",
    "\n",
    "И так, мы поняли, какую функцию ошибки будем минимизировать, разобрались, как получить предсказания по объекту и обученным весам. Осталось разобраться, как получить оптимальные веса. Для этого нужно выбрать какой-то метод оптимизации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск является самым популярным алгоритмом обучения линейных моделей. В этом задании Вам предложат реализовать стохастический градиентный спуск или  мини-батч градиентный спуск (мини-батч на русский язык довольно сложно перевести, многие переводят это как \"пакетный\", но мне не кажется этот перевод удачным). Далее нам потребуется определение **эпохи**.\n",
    "Эпохой в SGD и MB-GD называется один проход по **всем** объектам в обучающей выборки.\n",
    "* В SGD градиент расчитывается по одному случайному объекту. Сам алгоритм выглядит примерно так:\n",
    "        1) Перемешать выборку\n",
    "        2) Посчитать градиент функции потерь на одном объекте (далее один объект тоже будем называть батчем)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* В Mini Batch SGD - по подвыборке объектов. Сам алгоритм выглядит примерно так::\n",
    "        1) Перемешать выборку, выбрать размер мини-батча (от 1 до размера выборки)\n",
    "        2) Почитать градиент функции потерь по мини-батчу (не забыть поделить на  число объектов в мини-батче)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* Для отладки алгоритма реализуйте возможность  вывода средней ошибки на обучении модели по объектам (мини-батчам). После шага градиентного спуска посчитайте значение ошибки на объекте (или мини-батче), а затем усредните, например, по ста шагам. Если обучение проходит корректно, то мы должны увидеть, что каждые 100 шагов функция потерь уменьшается. \n",
    "* Правило останова - максимальное количество эпох\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические вопросы (2 балла)\n",
    "В этой части Вам будут предложены теоретичские вопросы и задачи по теме. Вы, конечно, можете списать их у своего товарища или найти решение в интернете, но учтите, что они обязательно войдут в теоретический коллоквиум. Лучше разобраться в теме сейчас и успешно ответить на коллоквиуме, чем списать, не разобравшись в материале, и быть терзаемым совестью. \n",
    "\n",
    "\n",
    "Формулы надо оформлять в формате **LaTeX**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 1. Градиент для линейной регрессии.\n",
    "* Выпишите формулу обновления весов для линейной регрессии с L2 регуляризацией для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_{new} = w_{old} - \\alpha \\cdot \\frac{2}{n} \\cdot \\sum_i^n \\left( \\left( y_i - \\sum_j x_{i_j} w_{old_j} \\right) \\cdot \\left( - \\sum_j x_{i_j} \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 2. Градиент для логистической регрессии.\n",
    "* Выпишите формулу обновления весов для логистической регрессии с L2 регуляризацией  для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент? Как соотносится этот градиент с градиентом, возникающий в задаче линейной регрессии?\n",
    "\n",
    "Подсказка: Вам градиент, которой получается если “в лоб” продифференцировать,  надо немного преобразовать.\n",
    "Надо подставить, что $1 - \\sigma(w,x) $ это  $1 - a(x_i)$, а  $-\\sigma(w,x)$ это $0 - a(x_i)$.  Тогда получится свести к одной красивой формуле с линейной регрессией, которую программировать будет намного проще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(a, X, Y) = - \\frac{1}{n} \\sum_i^n L(a, x_i, y_i) + \\sum_j w_j^2, x_i \\in X, y_i \\in Y$$\n",
    "$$L(w) = y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i)$$\n",
    "$$a_i = \\sigma( \\langle\\,x_i,w\\rangle ), \\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "$$ \\boldsymbol{w}_{new} = \\boldsymbol{w}_{old} - \\alpha \\nabla_w Q(w_{old})$$\n",
    "$$Q(w) = - \\frac{1}{n}\\left[\\sum_i y_i \\log \\frac{1}{1 + \\exp(-\\sum_j x_{i_j} w_j)} + ( 1 - y_i) \\log \\left(1 - \\frac{1}{1 + \\exp(- \\sum_j x_{i_j} w_j)}\\right) \\right] + \\sum_j w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_w Q = - \\frac{1}{n} \\cdot \\sum_i \\left[ - y_i \\frac{1}{ln2} \\left( 1 + exp\\left( - \\sum_j x_{i_j} w_{old_j} \\right) \\right)^{-1} exp\\left( -\\sum_j x_{i_j} w_{old_j} \\right) \\cdot (-1) \\cdot \\boldsymbol{x}_i + (1 - y_i) \\frac{1}{ln2} \\left( 1 - \\frac{1}{1 + exp\\left( - \\sum_j x_{i_j} w_{old_j} \\right)} \\right)^{-1} \\left( 1 + exp\\left( - \\sum_j x_{i_j} w_{old_j} \\right) \\right)^{-2} exp\\left( -\\sum_j x_{i_j} w_{old_j} \\right) \\cdot (-1) \\cdot \\boldsymbol{x}_i \\right] + 2 \\boldsymbol{w}_{old}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 3. Точное решение линейной регрессии\n",
    "\n",
    "На лекции было показано, что точное решение линейной регрессии имеет вид $w = (X^TX)^{-1}X^TY $. \n",
    "* Покажите, что это действительно является точкой минимума в случае, если матрица X имеет строк не меньше, чем столбцов и имеет полный ранг. Подсказка: посчитайте Гессиан и покажите, что в этом случае он положительно определен. \n",
    "* Выпишите точное решение для модели с $L2$ регуляризацией. Как L2 регуляризация помогает с точным решением где матрица X имеет линейно зависимые признаки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\boldsymbol{w}^* = (X^TX)^{-1}X^TY$ - точное решение уравнения $\\nabla_wQ=0$. Чтобы доказать, что это действительно является точкой минимума нужно проверить, что $H_w(Q(\\boldsymbol{w}^*)) \\succ 0$.\n",
    "$$Q(\\boldsymbol{w}) = \\frac{1}{N} || X \\cdot \\boldsymbol{w} - \\boldsymbol{y} ||^2 = \\frac{1}{N} (X \\cdot \\boldsymbol{w} - \\boldsymbol{y})^T (X \\cdot \\boldsymbol{w} - \\boldsymbol{y})$$\n",
    "$$\\nabla_w Q(\\boldsymbol{w}) = (X^TX + X^TX)\\boldsymbol{w} - 2X^T\\boldsymbol{y} = 2X^TX\\boldsymbol{w} - 2X^T\\boldsymbol{y}$$\n",
    "$$\\nabla_w(\\nabla_wQ(\\boldsymbol{w})) = 2X^TX$$\n",
    "Так как матрица $X$ имеет строк не меньше, чем столбцов и имеет полный ранг, а для положительной определённости необходимо чтобы $\\forall \\boldsymbol{z} \\in \\Re:$ $\\boldsymbol{z}^TX^TX\\boldsymbol{z} > 0$:\n",
    "$$\\boldsymbol{z}^TX^TX\\boldsymbol{z} = (X\\boldsymbol{z})^T(X\\boldsymbol{z}) = ||X\\boldsymbol{z}||^2 >0.$$\n",
    "* точное решение для $L_2$ регуляризации:\n",
    "$$\\boldsymbol{w} = (X^TX + \\lambda I)^{-1} X^T\\boldsymbol{y}.$$\n",
    "В случае, если у нас есть линейно зависимы признаки, то вес при них не зануляется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 4.  Предсказываем вероятности.\n",
    "\n",
    "Когда говорят о логистической регрессии, произносят фразу, что она \"предсказывает вероятности положительного класса\". Давайте разберемся, что же за этим стоит. Посчитаем математическое ожидание функции потерь и проверим, что предсказание алгоритма, оптимизирующее это мат. ожидание, будет являться вероятностью положительного класса. \n",
    "\n",
    "И так, функция потерь на объекте $x_i$, который имеет метку $y_i \\in \\{0,1\\}$  для предсказания $a(x_i)$ равна:\n",
    "$$L(y_i, b) =-[y_i == 1] \\log a(x_i)  - [y_i == 0] \\log(1 - a(x_i)) $$\n",
    "\n",
    "Где $[]$ означает индикатор $-$ он равен единице, если значение внутри него истинно, иначе он равен нулю. Тогда мат. ожидание при условии конкретного $x_i$  по определение мат. ожидания дискретной случайной величины:\n",
    "$$E(L | x_i) = -p(y_i = 1 |x_i ) \\log a(x_i)  - p(y_i = 0 | x_i) \\log( 1 - a(x_i))$$\n",
    "* Докажите, что значение $a(x_i)$, минимизирующее данное мат. ожидание, в точности равно $p(y_i = 1 |x_i)$, то есть равно вероятности положительного класса.\n",
    "\n",
    "Подсказка: возможно, придется воспользоваться, что  $p(y_i = 1 | x_i) + p(y_i = 0 | x_i) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 5.  Смысл регуляризации.\n",
    "\n",
    "Нужно ли в L1/L2 регуляризации использовать свободный член $w_0$ (который не умножается ни на какой признак)?\n",
    "\n",
    "Подсказка: подумайте, для чего мы вводим $w_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Реализация линейной модели (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужны батчи?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как Вы могли заметить из теоретического введения, что в случае SGD, что в случа mini-batch GD,  на каждой итерации обновление весов  происходит только по небольшой части данных (1 пример в случае SGD, batch примеров в случае mini-batch). То есть для каждой итерации нам *** не нужна вся выборка***. Мы можем просто итерироваться по выборке, беря батч нужного размера (далее 1 объект тоже будем называть батчом).\n",
    "\n",
    "Легко заметить, что в этом случае нам не нужно загружать все данные в оперативную память, достаточно просто считать батч с диска, обновить веса, считать диска другой батч и так далее. В целях упрощения домашней работы, прямо с диска  мы считывать не будем, будем работать с обычными numpy array. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Немножко про генераторы в Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея считывания данных кусками удачно ложится на так называемые ***генераторы*** из языка Python. В данной работе Вам предлагается не только разобраться с логистической регрессией, но  и познакомиться с таким важным элементом языка.  При желании Вы можете убрать весь код, связанный с генераторами, и реализовать логистическую регрессию и без них, ***штрафоваться это никак не будет***. Главное, чтобы сама модель была реализована правильно, и все пункты были выполнены. \n",
    "\n",
    "Подробнее можно почитать вот тут https://anandology.com/python-practice-book/iterators.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К генератору стоит относиться просто как к функции, которая порождает не один объект, а целую последовательность объектов. Новое значение из последовательности генерируется с помощью ключевого слова ***yield***. Ниже Вы можете насладиться  генератором чисел Фибоначчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(max_iter=4):\n",
    "    a, b = 0, 1\n",
    "    iter_num = 0\n",
    "    while 1:\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "        iter_num += 1\n",
    "        if iter_num == max_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так можно сгенерировать последовательность Фибоначчи. \n",
    "\n",
    "Заметьте, что к генераторам можно применять некоторые стандартные функции из Python, например enumerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "new_generator = fib()\n",
    "for j, fib_val in enumerate(new_generator):\n",
    "    print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пересоздавая объект, можно сколько угодно раз генерировать заново последовательность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n",
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n",
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    new_generator = fib()\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот так уже нельзя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "new_generator = fib()\n",
    "for i in range(0, 3):\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Концепция крайне удобная для обучения  моделей $-$ у Вас есть некий источник данных, который Вам выдает их кусками, и Вам совершенно все равно откуда он их берет. Под ним может скрывать как массив в оперативной памяти, как файл на жестком диске, так и SQL база данных. Вы сами данные никуда не сохраняете, оперативную память экономите."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если Вам понравилась идея с генераторами, то Вы можете реализовать свой, используя прототип batch_generator. В нем Вам нужно выдавать батчи признаков и ответов для каждой новой итерации спуска. Если не понравилась идея, то можете реализовывать SGD или mini-batch GD без генераторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y_batch    - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "    size = len(y)\n",
    "    ind = np.arange(size, dtype=int)\n",
    "    if shuffle==True:\n",
    "        np.random.shuffle(ind)\n",
    "    num = math.ceil(size / batch_size)\n",
    "    ind_batch = np.empty([num, batch_size], dtype=int)\n",
    "    for i in range(num):\n",
    "        tmp = np.arange(i*batch_size,(i+1)*batch_size, dtype=int)%size\n",
    "        ind_batch[i] = ind[tmp]\n",
    "    \n",
    "    x_size = X.shape\n",
    "    X_batch = np.empty([batch_size, x_size[1]])\n",
    "    y_batch = np.empty([batch_size])\n",
    "    for tmp_ind in ind_batch:\n",
    "        X_batch = X[tmp_ind, :]\n",
    "        y_batch = y[tmp_ind]\n",
    "        yield (X_batch, y_batch)\n",
    "\n",
    "# Теперь можно сделать генератор по данным ()\n",
    "#  my_batch_generator = batch_generator(X, y, shuffle=True, batch_size=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 6,  7,  8],\n",
      "       [ 9, 10, 11]]), array([2, 3]))\n",
      "(array([[ 3,  4,  5],\n",
      "       [12, 13, 14]]), array([1, 4]))\n",
      "(array([[0, 1, 2],\n",
      "       [6, 7, 8]]), array([0, 2]))\n",
      "[[ 1  0  1  2]\n",
      " [ 1  3  4  5]\n",
      " [ 1  6  7  8]\n",
      " [ 1  9 10 11]\n",
      " [ 1 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "X = np.arange(15).reshape((5,3))\n",
    "y = np.arange(5)\n",
    "gen = batch_generator(X, y, batch_size=2)\n",
    "for i in gen:\n",
    "    print(i)\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  3  6]\n",
      " [ 9 12 15]\n",
      " [18 21 24]\n",
      " [27 30 33]\n",
      " [36 39 42]]\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "X = np.arange(15).reshape((5,3))\n",
    "print(X*3)\n",
    "y = np.ones(3)\n",
    "print(np.dot(y,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоида.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "    \n",
    "    sigm_value_x = 1/(1+np.exp(-x))\n",
    "    return sigm_value_x\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, batch_generator, C=1, alpha=0.01, max_epoch=10, model_type='lin_reg'):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тим модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "        \n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter' : [], 'loss' : []}  \n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу \n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        y = np.array(y_batch)\n",
    "        if self.model_type == 'lin_reg':\n",
    "            a = np.matmul(X_batch, self.weights)\n",
    "            loss = np.dot((a-y),(a-y))/X_batch.shape[0] + np.dot(self.weights,self.weights)/self.C\n",
    "        elif self.model_type == 'log_reg':\n",
    "            a = sigmoid(np.matmul(X_batch,self.weights))\n",
    "            loss = -np.sum(y*np.log(a) + (1-y)*np.log(1-a))/X_batch.shape[0] +\\\n",
    "                np.dot(self.weights,self.weights)/self.C\n",
    "        else:\n",
    "            print('Error: no such model type')\n",
    "\n",
    "        loss -= - (self.weights[0]**2)/self.C\n",
    "        return loss\n",
    "    \n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем  градиент функции потерь по батчу (то что Вы вывели в задании 1)\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        y = np.array(y_batch)\n",
    "        if self.model_type == 'lin_reg':\n",
    "            loss_grad = 2*(np.matmul((np.matmul(X_batch.T, X_batch)), self.weights))/\\\n",
    "                X_batch.shape[0] + self.weights*2/self.C\n",
    "        elif self.model_type == 'log_reg':\n",
    "            a=sigmoid(np.matmul(X_batch,self.weights))\n",
    "            loss_grad = np.dot( X_batch.T, a-y)/X_batch.shape[0] +2*self.weights/self.C\n",
    "        else:\n",
    "            print('Error: no such model type')\n",
    "        loss_grad[0] -= self.weights[0]*2/self.C\n",
    "        return loss_grad\n",
    "    \n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"\n",
    "        self.weights -= self.alpha * new_grad\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''\n",
    "        \n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        self.min = np.amin(X, axis=0)\n",
    "        self.max = np.amax(X, axis=0)\n",
    "        # Нужно инициализровать случайно веса\n",
    "        self.weights = np.ones(X.shape[1], dtype=float)\n",
    "        for n in range(0, self.max_epoch):\n",
    "            new_epoch_generator = self.batch_generator(X, y, shuffle=True,\n",
    "                                                       batch_size=10)\n",
    "            for batch_num, new_batch in enumerate(new_epoch_generator):\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]\n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "                # Подумайте в каком месте стоит посчитать ошибку для отладки модели\n",
    "                # До градиентного шага или после\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''\n",
    "        \n",
    "        # Желательно здесь использовать матричные операции между X и весами, например, numpy.dot\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        y_hat = np.dot(X, self.weights)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите обе регрессии на синтетических данных. \n",
    "\n",
    "\n",
    "Выведите полученные веса и нарисуйте разделяющую границу между классами (используйте только первых два веса для первых двух признаков X[:,0], X[:,1] для отображения в 2d пространство ).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, c):\n",
    "    ## Your code Here\n",
    "    x = np.linspace(clf.min[1], clf.max[1], 100)\n",
    "    #x = np.arange(clf.min[1], clf.max[1], 0.01)\n",
    "    y = - (clf.weights[0] + clf.weights[1] * x) / clf.weights[2]\n",
    "    plt.plot(x,y, c=c)\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9a888f1c50>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "C1 = np.array([[0., -0.8], [1.5, 0.8]])\n",
    "C2 = np.array([[1., -0.7], [2., 0.7]])\n",
    "gauss1 = np.dot(np.random.randn(200, 2) + np.array([5, 3]), C1)\n",
    "gauss2 = np.dot(np.random.randn(200, 2) + np.array([1.5, 0]), C2)\n",
    "\n",
    "X = np.vstack([gauss1, gauss2])\n",
    "y = np.r_[np.ones(200), np.zeros(200)]\n",
    "\n",
    "lin = MySGDClassifier(batch_generator, C=1, alpha=0.01, max_epoch=10, model_type='lin_reg')\n",
    "lin.fit(X, y)\n",
    "#print(lin.errors_log)\n",
    "log = MySGDClassifier(batch_generator, C=1, alpha=0.01, max_epoch=10, model_type='log_reg')\n",
    "log.fit(X, y)\n",
    "#print(log.errors_log)\n",
    "plot_decision_boundary(lin, 'red')\n",
    "plot_decision_boundary(log, 'blue')\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем анализировать Ваш алгоритм. \n",
    "Для этих заданий используйте датасет ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100000, n_features=10, \n",
    "                           n_informative=4, n_redundant=0, \n",
    "                           random_state=123, class_sep=1.0,\n",
    "                           n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите сходимости обеих регрессией на этом датасете: изобразите график  функции потерь, усредненной по $N$ шагам градиентого спуска, для разных `alpha` (размеров шага). Разные `alpha` расположите на одном графике. \n",
    "\n",
    "$N$ можно брать 10, 50, 100 и т.д. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что Вы можете сказать про сходимость метода при различных `alpha`? Какое значение стоит выбирать для лучшей сходимости?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите график среднего значения весов для обеих регрессий в зависимости от коеф. регуляризации С из `np.logspace(3, -3, 10)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольны ли Вы, насколько сильно уменьшились Ваши веса? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Боевое применение (3  балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте применим модель на итоговом проекте! Датасет сделаем точно таким же образом, как было показано в project_overview.ipynb\n",
    "\n",
    "Применим обе регрессии, подберем для них параметры и сравним качество. Может быть Вы еще одновременно с решением домашней работы подрастете на лидерборде!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 15) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите размер батча для обучения. Линейная модель не должна учиться дольше нескольких минут. \n",
    "\n",
    "Не забывайте использовать скейлер!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте данные на обучение и валидацию. Подберите параметры C, alpha, max_epoch, model_type на валидации (Вы же помните, как правильно в этой задаче делать валидацию?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Подберите порог линейной модели, по достижении которого, Вы будете относить объект к классу 1. Вспомните, какую метрику мы оптимизируем в соревновании.  Как тогда правильно подобрать порог?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С лучшими параметрами на валидации сделайте предсказание на тестовом множестве, отправьте его на проверку на платформу kaggle. Убедитесь, что Вы смогли побить public score первого бейзлайна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** При сдаче домашки Вам необходимо кроме ссылки на ноутбук прислать Ваш ник на kaggle, под которым Вы залили решение, которое побило первый бейзлайн. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения линейных моделей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** ВАШ ОТЗЫВ ЗДЕСЬ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
